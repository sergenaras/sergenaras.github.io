Kubernetes, konteyner yönetimini bir orkestra şefi gibi düzenleyen bir yapıdır. Google tarafından Go dili ile geliştirilmiş ve sonrasında açık kaynaklı hale getirilmiştir. Sonrasında CNCF tarafından konteyner orkestrasyon standartı olarak ilan edilmiştir.  Kubernetes, ilk ve son karakterleri arasında 8 karakter olduğu için kısaltma olarak k8s olarak da bilinir.

## Temel Yapısı
K8s temel yapısından bahsetmek gerekirse; master ve worker olarak belirlenmiş iki temel yapı vardır. Master içerisinde yönetim için ihtiyaç duyulan api server, scheduler, controller manager ve etcd bileşenlerini barındırır. Worker ise içerisinde kube proxy ve kubelet araçlarını barındırır.  Bunların neden yaptığından bahsetmek gerekise; 

**API Server**, k8s içerisinde yukarıda bahsedilen bileşenler arasında biz sistem yöneticileri/geliştiriciler tarafından iletişim kurulan tek bileşen API Serverdır. Bir k8s cluster yapısı içerisindeki tüm yönetim işlemleri API Server üzerinden geçer ve yönetilir. 

**Controller Manager**, k8s cluster yapısı içerisindeki controller’ların asıl yöneticisidir. API Server’a gelen yaml dosyasındaki istenen durumun sağlanması ve kontrol edilmesi bu bileşenin sorumluluğudur. Birden fazla alt bileşeni vardır.

**Schedular**, cluster içerisindeki kaynak durumu, çalışma yoğunluğu gibi faktörleri bu bileşen tarafından kontrol ederiz. Bir pod, deployment vs. hangi worker üzerinde çalışması gerektiğini API Server’a söyler. 

**Etcd**, k8s cluster yapısında kayıt edilmesi gereken tüm veriler bu key-value şekilde çalışan veritabanına kayıt edilir. Tüm bileşenler arasında stateful olarak çalışan tek parçadır. 

<img src="{{ 'assets/pic/k8s/2022-05-23-k8s-kurulum-01.png' | relative_url }}" />

**Kube proxy**, cluster içerisindeki iletişimi sağlar.

**Kubelet**, worker node üzerinde API Server’dan gelen emirleri uygulayan yapıdır. 

**Container runtime**, konteyner’lerin çalışmasını sağlayan yazılımdır. Buna örnek olarak docker verilebilir. Ancak docker arkada containerd ile bir köprü görevi görmekte olduğu için CNCF 2020 yılında docker’ı denklemden çıkartmıştır. Kubernetes cluster yapısı herhangi bir container runtime ile kurulabilir ancak CNCF containerd önermektedir. 

Bir deployment’ın k8s cluster’ı içerisindeki geçtiği süreçler;

- Deployment yaml dosyası olarak hazırlanmıştır, kubectl komutu ile yürütülür.
- API Server talebi kontrol eder, kayıt edilmesi için etcd bileşenine iletir.
- Etcd kaydı gerçekleştirir, sonucu API Server’a gönderir.
- API Server oluşturulacak deployment için hangi worker’ın daha uygun olduğunu schedular’a sorar.
- Schedular worket node’ları karşılaştırır ve oluşturulacak deployment için uygun olan worker’ın bilgisini API Server’a iletir.
- API Server deployment oluşturulacak worker bilgisini Etcd bileşenine iletir.
- Etcd kayıt işlemini yapar ve sonucu API Server’a iletir.
- API Server ilgili node üzerindeki Kubelet ile iletişime geçer ve deployment’ı oluşturmasını ister.
- Kubelet deployment’ı ayağa kaldırır ve sonucu API Server’a gönderir.
- API Server deployment’ın oluştuğu bilgisini etcd’ye iletir.
- Etcd sonucu kayıt eder ve sonucu API Server’a iletir.
- API Server sonucu kubectl aracına çıktı olarak iletir.

Bir deployment/pod/service oluşturmak için izleyeceğimiz iki yol bulunmaktadır.

### Imperative
Bu şekilde bir şeyler oluştururken komut satırından tek tek komutlar yazmamız gerekmektedir. Küçük çaplı işlemler yaparken işimizi hızlandırır ancak bir proje için uygun değildir.

```
$ kubectl run demo-pod --image=nginx:alpine
```

### Declarative
Bu şekilde yapacağımız tanımlamalar bir dosya içerisinde, yaml formatında yazılmaktadır. Elimizde bir dosya olacağı için okuması ve ne olduğunu anlaması imperative yönteme göre daha kolaydır. Ancak yazması uzundur ve yaml oluştururken dikkat edilmesi gereken girintiler ve oluşturacağımız nesnelerin yaml kullanımını bilmemiz gerekmektedir.

```
$ cat basic-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: demo-deployment
spec:
  containers:
  - name: nginx-deployment
    image: nginx:alpine
```

### Genereting Manifests
Imperative olarak tanımlanan bir yapıyı declerative hale getirebiliriz. Bunu elle yapmak yerine çıktı olarak -o yaml kullanmamız yeterlidir.

```
# kubectl run --image=nginx nginx --dry-run=client -o yaml > demo-pod.yaml
# cat demo-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  containers:
  - image: nginx
    name: nginx
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```

Bu şekilde çalıştırılan bir komut çalışmayacak sadece çıktısını dosyaya aktaracaktır.

```
# kubectl get pod
No resources found in default namespace.
```

### Probe
Periyodik olarak cluster üzerinde yapılan tarama (diognostig) operasyonudur. Kubernetes pod’un durumunu probe işlemi ile öğrenir. Üç çeşit probe vardır:

**liveness**, “sağlıklı çalışıyor mu?” sorusuna cevap verir.

**readness**, “pod istek kabul edebiliyor mu?” sorusuna cevap verir.

**startup**, “container’daki uygulama başarılı mı?” sorusuna cevap verir.

Probe methodları ise;

**Execaction**, konteyner içerisinde bir şey gerçekleştirmek için kullanılır

**TCPSocketAction**, konteyner içerisinde IP ve Port adresini kontrol eder.

**HTTPGetAction**, HTTP isteği gerçekleştirmek için kullanılır.

Bu method’lar success, failure ve unknow şeklinde dönüşler alırız.

Kubernetes yapısı itibariyle kurulum öncesinde bazı ayarlara ihtiyacı duymaktatır. Bunlar sistem üzerine kurulacak olan parçaların birbirleri ile iletişimi, Kubernetes Cluster’ın kendi arasındaki iletişim konusunda önemli işlemlerdir.

## Kurulum
Temellerde daha fazla ilerlemeden kurulumu yapalım. Bunun için Ubuntu 20.04 LTS kullanacağız.

```
$ sudo apt-get update && sudo apt-get upgrade -y
```

Kurulum öncesinde sistemdeki takas alanını devre dışı bırakalım.

```
$ sudo sed -i 's/^\(.*swap\)/#\1/g' /etc/fstab
$ sudo swapoff -a
```

Sonrasında containerd için modül tanımlamalarını yapalım.

```
$ cat <<EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF
overlay
br_netfilter
```

Sonrasında bu modülleri aktif hale getirelim.

```
$ sudo modprobe overlay
$ sudo modprobe br_netfilter
```

Modüllerden gelen ayarlarla birlikte ağ tanımlarını yapalım.

```
$ cat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
```

Ayarları sisteme uygulayalım.

```
$ sudo sysctl --system
* Applying /etc/sysctl.d/10-console-messages.conf ...
kernel.printk = 4 4 1 7
* Applying /etc/sysctl.d/10-ipv6-privacy.conf ...
* Applying /etc/sysctl.d/10-kernel-hardening.conf ...
kernel.kptr_restrict = 1
* Applying /etc/sysctl.d/10-link-restrictions.conf ...
fs.protected_hardlinks = 1
fs.protected_symlinks = 1
* Applying /etc/sysctl.d/10-magic-sysrq.conf ...
kernel.sysrq = 176
* Applying /etc/sysctl.d/10-network-security.conf ...
net.ipv4.conf.default.rp_filter = 2
net.ipv4.conf.all.rp_filter = 2
* Applying /etc/sysctl.d/10-ptrace.conf ...
kernel.yama.ptrace_scope = 1
* Applying /etc/sysctl.d/10-zeropage.conf ...
vm.mmap_min_addr = 65536
* Applying /usr/lib/sysctl.d/50-default.conf ...
net.ipv4.conf.default.promote_secondaries = 1
sysctl: setting key "net.ipv4.conf.all.promote_secondaries": Invalid argument
net.ipv4.ping_group_range = 0 2147483647
net.core.default_qdisc = fq_codel
fs.protected_regular = 1
fs.protected_fifos = 1
* Applying /usr/lib/sysctl.d/50-pid-max.conf ...
kernel.pid_max = 4194304
* Applying /etc/sysctl.d/99-kubernetes-cri.conf ...
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
* Applying /etc/sysctl.d/99-sysctl.conf ...
* Applying /usr/lib/sysctl.d/protect-links.conf ...
fs.protected_fifos = 1
fs.protected_hardlinks = 1
fs.protected_regular = 2
fs.protected_symlinks = 1
* Applying /etc/sysctl.conf ...
```

Bu ayarlardan sonra containerd kurulumu yapabiliriz.

```
$ sudo apt-get install containerd -y
```

Sonrasında bir dizin oluşturalım.

```
$ sudo mkdir -p /etc/containerd
```

Bu dizinin içerisine ise konfigürasyon dosyalarını çıkartıp servisi yeniden başlatacağız.

```
$ sud sudo su -
# containerd config default | tee /etc/containerd/config.toml
disabled_plugins = []
imports = []
oom_score = 0
plugin_dir = ""
required_plugins = []
root = "/var/lib/containerd"
state = "/run/containerd"
version = 2

[cgroup]
  path = ""

[debug]
  address = ""
  format = ""
  gid = 0
  level = ""
  uid = 0

[grpc]
  address = "/run/containerd/containerd.sock"
  gid = 0
  max_recv_message_size = 16777216
  max_send_message_size = 16777216
  tcp_address = ""
  tcp_tls_cert = ""
  tcp_tls_key = ""
  uid = 0

[metrics]
  address = ""
  grpc_histogram = false

[plugins]

  [plugins."io.containerd.gc.v1.scheduler"]
    deletion_threshold = 0
    mutation_threshold = 100
    pause_threshold = 0.02
    schedule_delay = "0s"
    startup_delay = "100ms"

  [plugins."io.containerd.grpc.v1.cri"]
    disable_apparmor = false
    disable_cgroup = false
    disable_hugetlb_controller = true
    disable_proc_mount = false
    disable_tcp_service = true
    enable_selinux = false
    enable_tls_streaming = false
    ignore_image_defined_volumes = false
    max_concurrent_downloads = 3
    max_container_log_line_size = 16384
    netns_mounts_under_state_dir = false
    restrict_oom_score_adj = false
    sandbox_image = "k8s.gcr.io/pause:3.5"
    selinux_category_range = 1024
    stats_collect_period = 10
    stream_idle_timeout = "4h0m0s"
    stream_server_address = "127.0.0.1"
    stream_server_port = "0"
    systemd_cgroup = false
    tolerate_missing_hugetlb_controller = true
    unset_seccomp_profile = ""

    [plugins."io.containerd.grpc.v1.cri".cni]
      bin_dir = "/opt/cni/bin"
      conf_dir = "/etc/cni/net.d"
      conf_template = ""
      max_conf_num = 1

    [plugins."io.containerd.grpc.v1.cri".containerd]
      default_runtime_name = "runc"
      disable_snapshot_annotations = true
      discard_unpacked_layers = false
      no_pivot = false
      snapshotter = "overlayfs"

      [plugins."io.containerd.grpc.v1.cri".containerd.default_runtime]
        base_runtime_spec = ""
        container_annotations = []
        pod_annotations = []
        privileged_without_host_devices = false
        runtime_engine = ""
        runtime_root = ""
        runtime_type = ""

        [plugins."io.containerd.grpc.v1.cri".containerd.default_runtime.options]

      [plugins."io.containerd.grpc.v1.cri".containerd.runtimes]

        [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
          base_runtime_spec = ""
          container_annotations = []
          pod_annotations = []
          privileged_without_host_devices = false
          runtime_engine = ""
          runtime_root = ""
          runtime_type = "io.containerd.runc.v2"

          [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
            BinaryName = ""
            CriuImagePath = ""
            CriuPath = ""
            CriuWorkPath = ""
            IoGid = 0
            IoUid = 0
            NoNewKeyring = false
            NoPivotRoot = false
            Root = ""
            ShimCgroup = ""
            SystemdCgroup = false

      [plugins."io.containerd.grpc.v1.cri".containerd.untrusted_workload_runtime]
        base_runtime_spec = ""
        container_annotations = []
        pod_annotations = []
        privileged_without_host_devices = false
        runtime_engine = ""
        runtime_root = ""
        runtime_type = ""

        [plugins."io.containerd.grpc.v1.cri".containerd.untrusted_workload_runtime.options]

    [plugins."io.containerd.grpc.v1.cri".image_decryption]
      key_model = "node"

    [plugins."io.containerd.grpc.v1.cri".registry]
      config_path = ""

      [plugins."io.containerd.grpc.v1.cri".registry.auths]

      [plugins."io.containerd.grpc.v1.cri".registry.configs]

      [plugins."io.containerd.grpc.v1.cri".registry.headers]

      [plugins."io.containerd.grpc.v1.cri".registry.mirrors]

    [plugins."io.containerd.grpc.v1.cri".x509_key_pair_streaming]
      tls_cert_file = ""
      tls_key_file = ""

  [plugins."io.containerd.internal.v1.opt"]
    path = "/opt/containerd"

  [plugins."io.containerd.internal.v1.restart"]
    interval = "10s"

  [plugins."io.containerd.metadata.v1.bolt"]
    content_sharing_policy = "shared"

  [plugins."io.containerd.monitor.v1.cgroups"]
    no_prometheus = false

  [plugins."io.containerd.runtime.v1.linux"]
    no_shim = false
    runtime = "runc"
    runtime_root = ""
    shim = "containerd-shim"
    shim_debug = false

  [plugins."io.containerd.runtime.v2.task"]
    platforms = ["linux/amd64"]

  [plugins."io.containerd.service.v1.diff-service"]
    default = ["walking"]

  [plugins."io.containerd.snapshotter.v1.aufs"]
    root_path = ""

  [plugins."io.containerd.snapshotter.v1.btrfs"]
    root_path = ""

  [plugins."io.containerd.snapshotter.v1.devmapper"]
    async_remove = false
    base_image_size = ""
    pool_name = ""
    root_path = ""

  [plugins."io.containerd.snapshotter.v1.native"]
    root_path = ""

  [plugins."io.containerd.snapshotter.v1.overlayfs"]
    root_path = ""

  [plugins."io.containerd.snapshotter.v1.zfs"]
    root_path = ""

[proxy_plugins]

[stream_processors]

  [stream_processors."io.containerd.ocicrypt.decoder.v1.tar"]
    accepts = ["application/vnd.oci.image.layer.v1.tar+encrypted"]
    args = ["--decryption-keys-path", "/etc/containerd/ocicrypt/keys"]
    env = ["OCICRYPT_KEYPROVIDER_CONFIG=/etc/containerd/ocicrypt/ocicrypt_keyprovider.conf"]
    path = "ctd-decoder"
    returns = "application/vnd.oci.image.layer.v1.tar"

  [stream_processors."io.containerd.ocicrypt.decoder.v1.tar.gzip"]
    accepts = ["application/vnd.oci.image.layer.v1.tar+gzip+encrypted"]
    args = ["--decryption-keys-path", "/etc/containerd/ocicrypt/keys"]
    env = ["OCICRYPT_KEYPROVIDER_CONFIG=/etc/containerd/ocicrypt/ocicrypt_keyprovider.conf"]
    path = "ctd-decoder"
    returns = "application/vnd.oci.image.layer.v1.tar+gzip"

[timeouts]
  "io.containerd.timeout.shim.cleanup" = "5s"
  "io.containerd.timeout.shim.load" = "5s"
  "io.containerd.timeout.shim.shutdown" = "3s"
  "io.containerd.timeout.task.state" = "2s"

[ttrpc]
  address = ""
  gid = 0
  uid = 0

# exit
logout
$ sudo systemctl restart containerd
```

Linux üzerindeki iptables bridged ayarlarını tanımlamamızı sağlayacak modülleri aktif etmeliyiz.

```
$ cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
br_netfilter
EOF
br_netfilter
```

Sonrasında modül üzerindeki ayarları için aşağıdaki dosyayı oluşturmalıyız.

```
$ cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
```

Son olarak bu aşamayı tamamlamak için tekrar sysctl komutunu yürütelim.

```
$ sudo sysctl --system
* Applying /etc/sysctl.d/10-console-messages.conf ...
kernel.printk = 4 4 1 7
* Applying /etc/sysctl.d/10-ipv6-privacy.conf ...
* Applying /etc/sysctl.d/10-kernel-hardening.conf ...
kernel.kptr_restrict = 1
* Applying /etc/sysctl.d/10-link-restrictions.conf ...
fs.protected_hardlinks = 1
fs.protected_symlinks = 1
* Applying /etc/sysctl.d/10-magic-sysrq.conf ...
kernel.sysrq = 176
* Applying /etc/sysctl.d/10-network-security.conf ...
net.ipv4.conf.default.rp_filter = 2
net.ipv4.conf.all.rp_filter = 2
* Applying /etc/sysctl.d/10-ptrace.conf ...
kernel.yama.ptrace_scope = 1
* Applying /etc/sysctl.d/10-zeropage.conf ...
vm.mmap_min_addr = 65536
* Applying /usr/lib/sysctl.d/50-default.conf ...
net.ipv4.conf.default.promote_secondaries = 1
sysctl: setting key "net.ipv4.conf.all.promote_secondaries": Invalid argument
net.ipv4.ping_group_range = 0 2147483647
net.core.default_qdisc = fq_codel
fs.protected_regular = 1
fs.protected_fifos = 1
* Applying /usr/lib/sysctl.d/50-pid-max.conf ...
kernel.pid_max = 4194304
* Applying /etc/sysctl.d/99-kubernetes-cri.conf ...
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
* Applying /etc/sysctl.d/99-sysctl.conf ...
* Applying /etc/sysctl.d/k8s.conf ...
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
* Applying /usr/lib/sysctl.d/protect-links.conf ...
fs.protected_fifos = 1
fs.protected_hardlinks = 1
fs.protected_regular = 2
fs.protected_symlinks = 1
* Applying /etc/sysctl.conf ...
```

Kubernetes’in çalışmasında önceli olacak bazı paketleri kuralım.

```
$ sudo apt-get update
$ sudo apt-get install -y apt-transport-https ca-certificates curl
```

Kubernetes için paket doğrulama anahtarlarını indirelim.

```
$ sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg
```

Kubernetes paketlerini indirmek için depo tanımlaması yapalım.

```
$ echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list
```

Tüm bu işlemlerden sonra paketleri güncelleyip kurulumlarını yapabiliriz.

```
$ sudo apt-get update
$ sudo apt-get install -y kubelet kubeadm kubectl
```

Son olarak ise işletim sistemine gelecek güncellemeleri uygularken etkilenmemeleri için paketlere işaret koyalım.

```
$ sudo apt-mark hold kubelet kubeadm kubectl
kubelet set on hold.
kubeadm set on hold.
kubectl set on hold.
```

Şu aşamada kubernetes için ihtiyacımız olan node hazır.. ancak bu haliyle üzerinde sadece Kubernetes komponentleri kurulu bir sunucuya sahibiz. Bunu Kubernetes’e çevirmek için bir master belirlemeli ve worker olarak belirlediğimiz diğer sunucuları bu master sunucusuna eklemeliyiz.

Bunun için öncelikle master olarak belirlediğimiz sunucuya ihtiyaç duyduğumuz imajları çekelim.

```
$ sudo kubeadm config images pull
[config/images] Pulled k8s.gcr.io/kube-apiserver:v1.23.5
[config/images] Pulled k8s.gcr.io/kube-controller-manager:v1.23.5
[config/images] Pulled k8s.gcr.io/kube-scheduler:v1.23.5
[config/images] Pulled k8s.gcr.io/kube-proxy:v1.23.5
[config/images] Pulled k8s.gcr.io/pause:3.6
[config/images] Pulled k8s.gcr.io/etcd:3.5.1-0
[config/images] Pulled k8s.gcr.io/coredns/coredns:v1.8.6
```

Sonrasında bu imajlar kullanılarak sunucuyu master ilan edelim. Bunu yaparken bir network tanımı yapmamız gerekecek bunun için genel olarak 192.168.0.0./16 prefix’i kullanılır. Ayrıca api-server ve control plane tanımları için de IP tanımlarını burada yapmalıyız.

```
$ sudo kubeadm init --pod-network-cidr=192.168.0.0/16 --apiserver-advertise-address=192.168.88.20 --control-plane-endpoint=192.168.88.20
[init] Using Kubernetes version: v1.23.5
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Using existing ca certificate authority
[certs] Using existing apiserver certificate and key on disk
[certs] Using existing apiserver-kubelet-client certificate and key on disk
[certs] Using existing front-proxy-ca certificate authority
[certs] Using existing front-proxy-client certificate and key on disk
[certs] Using existing etcd/ca certificate authority
[certs] Using existing etcd/server certificate and key on disk
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [kube20 localhost] and IPs [192.168.88.20 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 17.502737 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.23" in namespace kube-system with the configuration for the kubelets in the cluster
NOTE: The "kubelet-config-1.23" naming of the kubelet ConfigMap is deprecated. Once the UnversionedKubeletConfigMap feature gate graduates to Beta the default name will become just "kubelet-config". Kubeadm upgrade will handle this transition transparently.
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node kube20 as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node kube20 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: zqy7z7.2r32el9ba071v6fo
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join 192.168.88.20:6443 --token zqy7z7.2r32el9ba071v6fo \
        --discovery-token-ca-cert-hash sha256:7633642e9a8d973c6bcc073776a85eb0e57d5f53a1d6f98e4fae8af02033fc55 \
        --control-plane

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.88.20:6443 --token zqy7z7.2r32el9ba071v6fo \
        --discovery-token-ca-cert-hash sha256:7633642e9a8d973c6bcc073776a85eb0e57d5f53a1d6f98e4fae8af02033fc55
```

Bu işlem tamamlandığında /etc/kubernetes/admin.conf içerisinde Kubernetes’e ulaşmamız için bir dosya oluşmuş olması gerekiyor. Bu dosyayı kullanmamız gerekiyor.

```
$ mkdir -p $HOME/.kube
$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
$ sudo chown $(id -u):$(id -g) $HOME/.kube/config

$ kubectl get node
NAME     STATUS     ROLES                  AGE   VERSION
kube20   NotReady   control-plane,master   97s   v1.23.5
```

Şu aşamadan sonra kubectl ile Kubernetes API server’a erişebiliriz. Ancak Cluster için hala hazır değiliz. Bunun için sisteme bir virtual network katmanı kurmamız gerekiyor. Bu katmanlar arasında en popüler olanı Project Calico’dur.

```
$ kubectl create -f https://docs.projectcalico.org/manifests/tigera-operator.yaml
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/apiservers.operator.tigera.io created
customresourcedefinition.apiextensions.k8s.io/imagesets.operator.tigera.io created
customresourcedefinition.apiextensions.k8s.io/installations.operator.tigera.io created
customresourcedefinition.apiextensions.k8s.io/tigerastatuses.operator.tigera.io created
namespace/tigera-operator created
Warning: policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
podsecuritypolicy.policy/tigera-operator created
serviceaccount/tigera-operator created
clusterrole.rbac.authorization.k8s.io/tigera-operator created
clusterrolebinding.rbac.authorization.k8s.io/tigera-operator created
deployment.apps/tigera-operator created

$ kubectl create -f https://docs.projectcalico.org/manifests/custom-resources.yaml
installation.operator.tigera.io/default created
apiserver.operator.tigera.io/default created
```

Bu aşamada diğer sunucuları eklemek için yazdığım bir [ansible playbook](https://github.com/sergenaras/k8s-deploy)‘u kullanacağım.

```
$ ansible-playbook worker-deploy.yaml -K
BECOME password:
Kubernetes version [latest]:

PLAY [K8S Worker Deploybook] ********************************************************

TASK [K8S Cluster network module] ***************************************************
changed: [kube21]
changed: [kube22]

TASK [K8S Cluster network sysctl] ***************************************************
changed: [kube22]
changed: [kube21]

TASK [K8S Cluster network configuration apply] **************************************
changed: [kube21]
changed: [kube22]

TASK [Containerd network module] ****************************************************
changed: [kube22]
changed: [kube21]

TASK [Containerd overlay module activation] *****************************************
changed: [kube22]
changed: [kube21]

TASK [Containerd br_netfilter module activation] ************************************
changed: [kube22]
changed: [kube21]

TASK [K8S Container Runtime Interface(cri) configuration] ***************************
changed: [kube21]
changed: [kube22]

TASK [Containerd network configuration apply] ***************************************
changed: [kube21]
changed: [kube22]

TASK [APT Update - Upgrade] *********************************************************
changed: [kube21]
changed: [kube22]

TASK [Install containerd] ***********************************************************
changed: [kube22]
changed: [kube21]

TASK [Create folder for containerd] *************************************************
changed: [kube22]
changed: [kube21]

TASK [Configuration for containerd] *************************************************
changed: [kube22]
changed: [kube21]

TASK [Restart containerd] ***********************************************************
changed: [kube22]
changed: [kube21]

TASK [APT Update] *******************************************************************
changed: [kube22]
changed: [kube21]

TASK [Pre-required packages] ********************************************************
changed: [kube22]
changed: [kube21]

TASK [K8S packages keys] ************************************************************
[WARNING]: Consider using the get_url or uri module rather than running 'curl'.  If you need to use command
because get_url or uri is insufficient you can add 'warn: false' to this command task or set
'command_warnings=False' in ansible.cfg to get rid of this message.
changed: [kube22]
changed: [kube21]

TASK [K8S APT repo addresses] *******************************************************
changed: [kube21]
changed: [kube22]

TASK [APT Update] *******************************************************************
changed: [kube21]
changed: [kube22]

TASK [Install kubelet] **************************************************************
changed: [kube22]
changed: [kube21]

TASK [Install kubeadm] **************************************************************
changed: [kube21]
changed: [kube22]

TASK [Install kubectl] **************************************************************
ok: [kube21]
ok: [kube22]

TASK [Hold kubelet] *****************************************************************
changed: [kube22]
changed: [kube21]

TASK [Hold kubeadm] *****************************************************************
changed: [kube22]
changed: [kube21]

TASK [Hold kubectl] *****************************************************************
changed: [kube22]
changed: [kube21]

TASK [Remove dependencies that are no longer required] ******************************
ok: [kube22]
ok: [kube21]

TASK [Remove useless packages from the cache] ***************************************
ok: [kube22]
ok: [kube21]

PLAY RECAP **************************************************************************
kube21 : ok=26   changed=23  unreachable=0 failed=0  skipped=0  rescued=0   ignored=0
kube22 : ok=26   changed=23  unreachable=0 failed=0  skipped=0  rescued=0   ignored=0
```

Şu aşamada hızlı bir şekilde her iki sunucuya da Kubernetes node’ları kuruldu. Şimdi bu worker olacak sunuculardan birisine kubeadm init komutu sonunda aldığımız kubeadm join komutunu yürütelim.

```
$ sudo kubeadm join 192.168.88.20:6443 --token zqy7z7.2r32el9ba071v6fo \
>         --discovery-token-ca-cert-hash sha256:7633642e9a8d973c6bcc073776a85eb0e57d5f53a1d6f98e4fae8af02033fc55
[sudo] password for sergen:
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
W0418 11:43:47.821567   31784 utils.go:69] The recommended value for "resolvConf" in "KubeletConfiguration" is: /run/systemd/resolve/resolv.conf; the provided value is: /run/systemd/resolve/resolv.conf
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
```

Şimdi master sunucumuza dönüp komutun çıktısının sonunda önerdiği gibi kubectl get node komutunu yürütelim.

```
$ kubectl get node
NAME     STATUS     ROLES                  AGE     VERSION
kube20   Ready      control-plane,master   11m     v1.23.5
kube21   Ready      <none>                 4m29s   v1.23.5
kube22   NotReady   <none>                 32s     v1.23.5
```

Eğer Project Calico’yu kurmamış olsaydık status asla Ready konuma gelmeyecekti ancak yukarıda kurulumunu sağladığımız için süreç içerisinde NotReady olarak gelen sunucular bir kaç dakika içerisinde Ready konuma gelecektir.

Kubernetes Cluster yapımızı kurmuş olduk.
